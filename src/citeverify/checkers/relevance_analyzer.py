# -*- coding: utf-8 -*-
"""
å¼•æ–‡ç›¸å…³æ€§åˆ†æå™¨

ä½¿ç”¨ LLM åˆ†æå¼•ç”¨æ–‡æœ¬ä¸å‚è€ƒæ–‡çŒ®ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
åˆ¤æ–­å‚è€ƒæ–‡çŒ®æ˜¯å¦æ”¯æŒå¼•ç”¨å¤„çš„è®ºç‚¹ã€‚
"""
import re
import json
import logging
import concurrent.futures
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Tuple
from enum import Enum

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

logger = logging.getLogger(__name__)


class RelevanceJudgment(Enum):
    """ç›¸å…³æ€§åˆ¤æ–­ç»“æœ"""
    STRONGLY_SUPPORTS = "strongly_supports"
    WEAKLY_SUPPORTS = "weakly_supports"
    DOES_NOT_SUPPORT = "does_not_support"
    UNCLEAR = "unclear"
    ERROR = "error"  # åˆ†æå¤±è´¥
    
    @classmethod
    def from_string(cls, text: str) -> 'RelevanceJudgment':
        """ä»å­—ç¬¦ä¸²è§£æåˆ¤æ–­ç»“æœ"""
        text_lower = text.lower().strip()
        
        if 'strongly support' in text_lower:
            return cls.STRONGLY_SUPPORTS
        elif 'weakly support' in text_lower:
            return cls.WEAKLY_SUPPORTS
        elif 'does not support' in text_lower or 'not support' in text_lower:
            return cls.DOES_NOT_SUPPORT
        elif 'unclear' in text_lower:
            return cls.UNCLEAR
        else:
            return cls.UNCLEAR  # é»˜è®¤è¿”å› unclear


@dataclass
class RelevanceResult:
    """ç›¸å…³æ€§åˆ†æç»“æœ"""
    # è¾“å…¥ä¿¡æ¯
    title: str                          # å‚è€ƒæ–‡çŒ®æ ‡é¢˜
    abstract: str                       # å‚è€ƒæ–‡çŒ®æ‘˜è¦
    citation_anchor: str                # å¼•ç”¨æ‰€åœ¨å¥å­
    context: str                        # å¼•ç”¨ä¸Šä¸‹æ–‡
    
    # åˆ†æç»“æœ
    claim: str                          # æ¨æ–­çš„è®ºç‚¹
    judgment: RelevanceJudgment         # ç›¸å…³æ€§åˆ¤æ–­
    reason: str                         # åˆ¤æ–­ç†ç”±
    
    # å…ƒä¿¡æ¯
    raw_response: str = ""              # LLM åŸå§‹å“åº”
    success: bool = True                # æ˜¯å¦åˆ†ææˆåŠŸ
    error_message: str = ""             # é”™è¯¯ä¿¡æ¯
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "title": self.title,
            "abstract": self.abstract[:200] + "..." if len(self.abstract) > 200 else self.abstract,
            "citation_anchor": self.citation_anchor,
            "context": self.context,
            "claim": self.claim,
            "judgment": self.judgment.value,
            "reason": self.reason,
            "success": self.success,
            "error_message": self.error_message,
        }
    
    def to_list(self) -> List:
        """è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼"""
        return [
            self.title,
            self.citation_anchor,
            self.claim,
            self.judgment.value,
            self.reason,
            self.success,
        ]


# æç¤ºè¯æ¨¡æ¿
RELEVANCE_PROMPT_TEMPLATE = """# Role
You are an academic citation verification assistant.

# Task
Your task is to determine whether a given reference paper supports the claim made at a specific citation point in a manuscript.

# Input
Reference title: {title}
Reference abstract: {abstract}
Citation anchor: {citation_anchor}
Context: {context}

# Instructions
You must follow the steps below strictly and must NOT introduce any information that is not explicitly stated in the provided text.

Step 1:  
Based only on the citation anchor and its surrounding context, infer the claim that the authors intend to make at this citation point.  
If the claim involves reference, omission, comparison, or negation, use the context to clarify it.  
If no clear claim can be inferred, state that explicitly.

Step 2:  
Based only on the reference title and abstract, determine whether the reference supports the inferred claim.

Your judgment must be one of the following four categories:
- Strongly supports
- Weakly supports
- Does not support
- Unclear

Provide a brief reason for your judgment, strictly grounded in the given texts.

# Output Format
You MUST respond with a valid JSON object in the following format (no other text before or after):
```json
{{
    "claim": "The inferred claim from the citation context",
    "judge": "One of: Strongly supports / Weakly supports / Does not support / Unclear",
    "reason": "Brief reason for the judgment"
}}
```
"""


class RelevanceAnalyzer:
    """
    å¼•æ–‡ç›¸å…³æ€§åˆ†æå™¨
    
    ä½¿ç”¨ LLM åˆ†æå¼•ç”¨æ–‡æœ¬ä¸å‚è€ƒæ–‡çŒ®ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
    """
    
    def __init__(
        self,
        model_name: str = "gpt-4o-mini",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        timeout: float = 60.0,
        max_retries: int = 2,
    ):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-4o-mini", "gpt-4o", "deepseek-chat" ç­‰ï¼‰
            api_key: API å¯†é’¥
            base_url: API åŸºç¡€ URLï¼ˆç”¨äºå…¼å®¹å…¶ä»– OpenAI é£æ ¼çš„ APIï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        if OpenAI is None:
            raise ImportError(
                "openai åŒ…æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install openai"
            )
        
        self.model_name = model_name
        self.api_key = api_key
        self.base_url = base_url
        self.timeout = timeout
        self.max_retries = max_retries
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        client_kwargs = {}
        if api_key:
            client_kwargs["api_key"] = api_key
        if base_url:
            client_kwargs["base_url"] = base_url
        if timeout:
            client_kwargs["timeout"] = timeout
        
        self.client = OpenAI(**client_kwargs)
    
    def _build_prompt(
        self,
        title: str,
        abstract: str,
        citation_anchor: str,
        context: str,
    ) -> str:
        """
        æ„å»ºæç¤ºè¯
        
        Args:
            title: å‚è€ƒæ–‡çŒ®æ ‡é¢˜
            abstract: å‚è€ƒæ–‡çŒ®æ‘˜è¦
            citation_anchor: å¼•ç”¨æ‰€åœ¨å¥å­
            context: å¼•ç”¨ä¸Šä¸‹æ–‡
            
        Returns:
            æ ¼å¼åŒ–åçš„æç¤ºè¯
        """
        return RELEVANCE_PROMPT_TEMPLATE.format(
            title=title,
            abstract=abstract,
            citation_anchor=citation_anchor,
            context=context,
        )
    
    def _parse_response(self, response_text: str) -> Tuple[str, RelevanceJudgment, str]:
        """
        è§£æ LLM å“åº”ï¼ˆJSON æ ¼å¼ï¼‰
        
        Args:
            response_text: LLM çš„åŸå§‹å“åº”æ–‡æœ¬
            
        Returns:
            (claim, judgment, reason)
        """
        claim = ""
        judgment = RelevanceJudgment.UNCLEAR
        reason = ""
        
        # å°è¯•æå– JSON å—
        # 1. å…ˆå°è¯•æå– ```json ... ``` ä»£ç å—
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1).strip()
        else:
            # 2. å°è¯•ç›´æ¥æŸ¥æ‰¾ JSON å¯¹è±¡
            json_match = re.search(r'\{[^{}]*"claim"[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                # 3. å›é€€ï¼šå°è¯•æ•´ä¸ªå“åº”ä½œä¸º JSON
                json_str = response_text.strip()
        
        # è§£æ JSON
        try:
            data = json.loads(json_str)
            claim = data.get("claim", "")
            judge_text = data.get("judge", "")
            reason = data.get("reason", "")
            
            if judge_text:
                judgment = RelevanceJudgment.from_string(judge_text)
            
        except json.JSONDecodeError:
            # JSON è§£æå¤±è´¥ï¼Œå›é€€åˆ°æ­£åˆ™è§£æï¼ˆå…¼å®¹æ€§ï¼‰
            claim_match = re.search(
                r'["\']?claim["\']?\s*[:ï¼š]\s*["\']?(.+?)["\']?\s*[,}]',
                response_text,
                re.DOTALL | re.IGNORECASE
            )
            if claim_match:
                claim = claim_match.group(1).strip().strip('"\'')
            
            judge_match = re.search(
                r'["\']?judge["\']?\s*[:ï¼š]\s*["\']?(.+?)["\']?\s*[,}]',
                response_text,
                re.DOTALL | re.IGNORECASE
            )
            if judge_match:
                judge_text = judge_match.group(1).strip().strip('"\'')
                judgment = RelevanceJudgment.from_string(judge_text)
            
            reason_match = re.search(
                r'["\']?reason["\']?\s*[:ï¼š]\s*["\']?(.+?)["\']?\s*}',
                response_text,
                re.DOTALL | re.IGNORECASE
            )
            if reason_match:
                reason = reason_match.group(1).strip().strip('"\'')
        
        return claim, judgment, reason
    
    def analyze(
        self,
        title: str,
        abstract: str,
        citation_anchor: str,
        context: str,
        temperature: float = 0.1,
    ) -> RelevanceResult:
        """
        åˆ†æå•ä¸ªå¼•ç”¨çš„ç›¸å…³æ€§
        
        Args:
            title: å‚è€ƒæ–‡çŒ®æ ‡é¢˜
            abstract: å‚è€ƒæ–‡çŒ®æ‘˜è¦
            citation_anchor: å¼•ç”¨æ‰€åœ¨å¥å­
            context: å¼•ç”¨ä¸Šä¸‹æ–‡
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆè¶Šä½è¶Šç¡®å®šæ€§ï¼‰
            
        Returns:
            ç›¸å…³æ€§åˆ†æç»“æœ
        """
        # æ£€æŸ¥è¾“å…¥
        if not title:
            return RelevanceResult(
                title=title,
                abstract=abstract,
                citation_anchor=citation_anchor,
                context=context,
                claim="",
                judgment=RelevanceJudgment.ERROR,
                reason="",
                success=False,
                error_message="å‚è€ƒæ–‡çŒ®æ ‡é¢˜ä¸ºç©º",
            )
        
        if not abstract:
            return RelevanceResult(
                title=title,
                abstract=abstract,
                citation_anchor=citation_anchor,
                context=context,
                claim="",
                judgment=RelevanceJudgment.UNCLEAR,
                reason="æ— æ³•åˆ†æï¼šå‚è€ƒæ–‡çŒ®æ‘˜è¦ä¸ºç©º",
                success=True,
            )
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(title, abstract, citation_anchor, context)
        
        # è°ƒç”¨ LLM
        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are an academic citation verification assistant. Respond in the exact format requested."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=temperature,
                    max_tokens=1024,
                )
                
                # æå–å“åº”æ–‡æœ¬
                raw_response = response.choices[0].message.content
                
                # è§£æå“åº”
                claim, judgment, reason = self._parse_response(raw_response)
                
                return RelevanceResult(
                    title=title,
                    abstract=abstract,
                    citation_anchor=citation_anchor,
                    context=context,
                    claim=claim,
                    judgment=judgment,
                    reason=reason,
                    raw_response=raw_response,
                    success=True,
                )
                
            except Exception as e:
                last_error = e
                if attempt < self.max_retries:
                    continue
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return RelevanceResult(
            title=title,
            abstract=abstract,
            citation_anchor=citation_anchor,
            context=context,
            claim="",
            judgment=RelevanceJudgment.ERROR,
            reason="",
            raw_response="",
            success=False,
            error_message=f"LLM è°ƒç”¨å¤±è´¥: {str(last_error)}",
        )
    
    def analyze_batch(
        self,
        items: List[Dict[str, str]],
        temperature: float = 0.1,
        progress_callback: Optional[callable] = None,
        max_workers: int = 5,
    ) -> List[RelevanceResult]:
        """
        æ‰¹é‡å¹¶è¡Œåˆ†æå¼•ç”¨ç›¸å…³æ€§
        
        Args:
            items: å¾…åˆ†æé¡¹åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
                - title: å‚è€ƒæ–‡çŒ®æ ‡é¢˜
                - abstract: å‚è€ƒæ–‡çŒ®æ‘˜è¦
                - citation_anchor: å¼•ç”¨æ‰€åœ¨å¥å­
                - context: å¼•ç”¨ä¸Šä¸‹æ–‡
            temperature: ç”Ÿæˆæ¸©åº¦
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç­¾åä¸º callback(current, total)
            max_workers: æœ€å¤§å¹¶è¡Œæ•°ï¼ˆé»˜è®¤ 5ï¼‰
            
        Returns:
            ç›¸å…³æ€§åˆ†æç»“æœåˆ—è¡¨ï¼ˆæŒ‰åŸå§‹é¡ºåºï¼‰
        """
        total = len(items)
        if total == 0:
            return []
        
        # å­˜å‚¨ç»“æœï¼ˆæŒ‰åŸå§‹ç´¢å¼•ï¼‰
        results = [None] * total
        completed = 0
        
        def analyze_single(idx_item: Tuple[int, Dict[str, str]]) -> Tuple[int, RelevanceResult]:
            """åˆ†æå•ä¸ªé¡¹ç›®"""
            idx, item = idx_item
            result = self.analyze(
                title=item.get("title", ""),
                abstract=item.get("abstract", ""),
                citation_anchor=item.get("citation_anchor", ""),
                context=item.get("context", ""),
                temperature=temperature,
            )
            return idx, result
        
        # å‡†å¤‡å¸¦ç´¢å¼•çš„ä»»åŠ¡
        indexed_items = list(enumerate(items))
        
        # å¹¶è¡Œæ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(analyze_single, item): item[0] for item in indexed_items}
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    idx, result = future.result()
                    results[idx] = result
                    completed += 1
                    
                    logger.debug(f"ç›¸å…³æ€§åˆ†æ [{completed}/{total}] å®Œæˆ: {result.title[:30] if result.title else 'N/A'}...")
                    
                    if progress_callback:
                        progress_callback(completed, total)
                        
                except Exception as e:
                    idx = futures[future]
                    logger.warning(f"ç›¸å…³æ€§åˆ†æ [{idx}] å¤±è´¥: {e}")
                    results[idx] = RelevanceResult(
                        title=items[idx].get("title", ""),
                        citation_anchor=items[idx].get("citation_anchor", ""),
                        context=items[idx].get("context", ""),
                        judgment=RelevanceJudgment.ERROR,
                        success=False,
                        error_message=str(e),
                    )
                    completed += 1
                    if progress_callback:
                        progress_callback(completed, total)
        
        return results
    
    def analyze_matched_citations(
        self,
        matched_citations: List,
        temperature: float = 0.1,
        progress_callback: Optional[callable] = None,
        max_workers: int = 5,
    ) -> List[RelevanceResult]:
        """
        åˆ†æåŒ¹é…åçš„å¼•ç”¨åˆ—è¡¨ï¼ˆå¹¶è¡Œï¼‰
        
        æ¥æ”¶ CitationMatcher çš„è¾“å‡ºï¼Œè¿›è¡Œç›¸å…³æ€§åˆ†æã€‚
        
        Args:
            matched_citations: åŒ¹é…çš„å¼•ç”¨åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º MatchedCitation æˆ–
                [title, authors, year, abstract, pdf_url, citation_anchor, context, match_score]
            temperature: ç”Ÿæˆæ¸©åº¦
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            max_workers: æœ€å¤§å¹¶è¡Œæ•°ï¼ˆé»˜è®¤ 5ï¼‰
            
        Returns:
            ç›¸å…³æ€§åˆ†æç»“æœåˆ—è¡¨
        """
        items = []
        
        for citation in matched_citations:
            # å¤„ç†åˆ—è¡¨æ ¼å¼
            if isinstance(citation, (list, tuple)):
                title = citation[0] if len(citation) > 0 else ""
                abstract = citation[3] if len(citation) > 3 else ""
                citation_anchor = citation[5] if len(citation) > 5 else ""
                context = citation[6] if len(citation) > 6 else ""
            # å¤„ç† MatchedCitation å¯¹è±¡
            elif hasattr(citation, 'title'):
                title = citation.title
                abstract = getattr(citation, 'abstract', "")
                citation_anchor = citation.citation_anchor
                context = citation.context
            else:
                continue
            
            items.append({
                "title": title,
                "abstract": abstract,
                "citation_anchor": citation_anchor,
                "context": context,
            })
        
        return self.analyze_batch(items, temperature, progress_callback, max_workers)


# ================== ä¾¿æ·å‡½æ•° ==================

def analyze_relevance(
    title: str,
    abstract: str,
    citation_anchor: str,
    context: str,
    model_name: str = "gpt-4o-mini",
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: float = 0.1,
) -> RelevanceResult:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ†æå•ä¸ªå¼•ç”¨çš„ç›¸å…³æ€§
    
    Args:
        title: å‚è€ƒæ–‡çŒ®æ ‡é¢˜
        abstract: å‚è€ƒæ–‡çŒ®æ‘˜è¦
        citation_anchor: å¼•ç”¨æ‰€åœ¨å¥å­
        context: å¼•ç”¨ä¸Šä¸‹æ–‡
        model_name: æ¨¡å‹åç§°
        api_key: API å¯†é’¥
        base_url: API åŸºç¡€ URL
        temperature: ç”Ÿæˆæ¸©åº¦
        
    Returns:
        ç›¸å…³æ€§åˆ†æç»“æœ
    """
    analyzer = RelevanceAnalyzer(
        model_name=model_name,
        api_key=api_key,
        base_url=base_url,
    )
    return analyzer.analyze(title, abstract, citation_anchor, context, temperature)


def analyze_relevance_batch(
    items: List[Dict[str, str]],
    model_name: str = "gpt-4o-mini",
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: float = 0.1,
    progress_callback: Optional[callable] = None,
    max_workers: int = 5,
) -> List[RelevanceResult]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡å¹¶è¡Œåˆ†æå¼•ç”¨ç›¸å…³æ€§
    
    Args:
        items: å¾…åˆ†æé¡¹åˆ—è¡¨
        model_name: æ¨¡å‹åç§°
        api_key: API å¯†é’¥
        base_url: API åŸºç¡€ URL
        temperature: ç”Ÿæˆæ¸©åº¦
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        max_workers: æœ€å¤§å¹¶è¡Œæ•°ï¼ˆé»˜è®¤ 5ï¼‰
        
    Returns:
        ç›¸å…³æ€§åˆ†æç»“æœåˆ—è¡¨
    """
    analyzer = RelevanceAnalyzer(
        model_name=model_name,
        api_key=api_key,
        base_url=base_url,
    )
    return analyzer.analyze_batch(items, temperature, progress_callback, max_workers)


def generate_relevance_report(results: List[RelevanceResult]) -> str:
    """
    ç”Ÿæˆç›¸å…³æ€§åˆ†ææŠ¥å‘Š
    
    Args:
        results: åˆ†æç»“æœåˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
    """
    lines = []
    lines.append("=" * 70)
    lines.append("å¼•æ–‡ç›¸å…³æ€§åˆ†ææŠ¥å‘Š")
    lines.append("=" * 70)
    
    # ç»Ÿè®¡
    total = len(results)
    success_count = sum(1 for r in results if r.success)
    
    judgment_counts = {
        RelevanceJudgment.STRONGLY_SUPPORTS: 0,
        RelevanceJudgment.WEAKLY_SUPPORTS: 0,
        RelevanceJudgment.DOES_NOT_SUPPORT: 0,
        RelevanceJudgment.UNCLEAR: 0,
        RelevanceJudgment.ERROR: 0,
    }
    for r in results:
        judgment_counts[r.judgment] += 1
    
    lines.append(f"\næ€»è®¡: {total} æ¡å¼•ç”¨")
    lines.append(f"æˆåŠŸåˆ†æ: {success_count}")
    lines.append(f"åˆ†æå¤±è´¥: {total - success_count}")
    lines.append("")
    lines.append("åˆ¤æ–­åˆ†å¸ƒ:")
    lines.append(f"  - å¼ºæ”¯æŒ (Strongly supports): {judgment_counts[RelevanceJudgment.STRONGLY_SUPPORTS]}")
    lines.append(f"  - å¼±æ”¯æŒ (Weakly supports): {judgment_counts[RelevanceJudgment.WEAKLY_SUPPORTS]}")
    lines.append(f"  - ä¸æ”¯æŒ (Does not support): {judgment_counts[RelevanceJudgment.DOES_NOT_SUPPORT]}")
    lines.append(f"  - ä¸ç¡®å®š (Unclear): {judgment_counts[RelevanceJudgment.UNCLEAR]}")
    lines.append(f"  - é”™è¯¯ (Error): {judgment_counts[RelevanceJudgment.ERROR]}")
    
    lines.append("\n" + "-" * 70)
    lines.append("è¯¦ç»†ç»“æœ:")
    lines.append("-" * 70)
    
    for i, r in enumerate(results, 1):
        lines.append(f"\n[{i}] {r.title[:50]}...")
        lines.append(f"    å¼•ç”¨å¥: {r.citation_anchor[:60]}...")
        lines.append(f"    æ¨æ–­è®ºç‚¹: {r.claim[:80]}..." if r.claim else "    æ¨æ–­è®ºç‚¹: (æ— )")
        
        # åˆ¤æ–­ç»“æœä½¿ç”¨ä¸åŒæ ‡è®°
        judgment_marks = {
            RelevanceJudgment.STRONGLY_SUPPORTS: "âœ… å¼ºæ”¯æŒ",
            RelevanceJudgment.WEAKLY_SUPPORTS: "ğŸ”¶ å¼±æ”¯æŒ",
            RelevanceJudgment.DOES_NOT_SUPPORT: "âŒ ä¸æ”¯æŒ",
            RelevanceJudgment.UNCLEAR: "â“ ä¸ç¡®å®š",
            RelevanceJudgment.ERROR: "âš ï¸ é”™è¯¯",
        }
        lines.append(f"    åˆ¤æ–­: {judgment_marks.get(r.judgment, r.judgment.value)}")
        lines.append(f"    ç†ç”±: {r.reason[:100]}..." if r.reason else "    ç†ç”±: (æ— )")
        
        if not r.success:
            lines.append(f"    é”™è¯¯: {r.error_message}")
    
    lines.append("\n" + "=" * 70)
    
    return "\n".join(lines)
