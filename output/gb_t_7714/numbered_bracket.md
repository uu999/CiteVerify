# 参考文献

[1] Vaswani A, 等. Attention is all you need[J]. Advances in Neural Information Processing Systems, 2017, 30: 5998-6008.

[2] Devlin J, 等. BERT: Pre-training of deep bidirectional transformers for language understanding[J]. Proceedings of NAACL-HLT, 2019, 1: 4171-4186.

[3] Brown T, 等. Language models are few-shot learners[J]. Advances in Neural Information Processing Systems, 2020, 33: 1877-1901.

[4] Liu Y, 等. RoBERTa: A robustly optimized BERT pretraining approach[J]. arXiv preprint, 2019, —: arXiv:1907.11692.

[5] Raffel C, 等. Exploring the limits of transfer learning with a unified text-to-text transformer[J]. Journal of Machine Learning Research, 2020, 21: 1-67.

[6] Lewis M, 等. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension[J]. Proceedings of the 58th Annual Meeting of the ACL, 2020, —: 7871-7880.

[7] Radford A, 等. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1: 9.

[8] Touvron H, 等. LLaMA: Open and efficient foundation language models[J]. arXiv preprint, 2023, —: arXiv:2302.13971.

[9] 李明, 王华, 张伟. 基于深度学习的自然语言处理综述[J]. 计算机学报, 2022, 45(3): 542-560.

[10] 刘强, 陈静. 大语言模型在信息检索中的应用[J]. 软件学报, 2023, 34(2): 801-820.
