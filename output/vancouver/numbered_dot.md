# 参考文献

1. Vaswani A,  Shazeer N et al.. Attention is all you need. Advances in Neural Information Processing Systems. 2017;30:5998-6008.

2. Devlin J,  Chang M-W et al.. BERT: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. 2019;1:4171-4186.

3. Brown T,  Mann B et al.. Language models are few-shot learners. Advances in Neural Information Processing Systems. 2020;33:1877-1901.

4. Liu Y,  Ott M et al.. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint. 2019;1:arXiv:1907.11692.

5. Raffel C,  Shazeer N et al.. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research. 2020;21:1-67.

6. Lewis M,  Liu Y et al.. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Proceedings of the 58th Annual Meeting of the ACL. 2020;1:7871-7880.

7. Radford A,  Wu J et al.. Language models are unsupervised multitask learners. OpenAI blog. 2019;1:9.

8. Touvron H,  Lavril T et al.. LLaMA: Open and efficient foundation language models. arXiv preprint. 2023;1:arXiv:2302.13971.

9. Achiam J,  Adler S et al.. GPT-4 technical report. arXiv preprint. 2023;1:arXiv:2303.08774.

10. Ouyang L,  Wu J et al.. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems. 2022;35:27730-27744.
