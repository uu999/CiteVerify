# 参考文献

[1] A, Vaswani et al. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30, pp. 5998-6008 https://doi.org/10.48550/arXiv.1706.03762.

[2] J, Devlin et al. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. *Proceedings of NAACL-HLT*, 1, pp. 4171-4186 https://doi.org/10.18653/v1/N19-1423.

[3] T, Brown et al. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, 33, pp. 1877-1901 https://doi.org/10.48550/arXiv.2005.14165.

[4] Y, Liu et al. (2019). RoBERTa: A robustly optimized BERT pretraining approach. *arXiv preprint*, pp. arXiv:1907.11692 https://doi.org/10.48550/arXiv.1907.11692.

[5] C, Raffel et al. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. *Journal of Machine Learning Research*, 21, pp. 1-67 https://doi.org/10.5555/3455716.3455856.

[6] M, Lewis et al. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. *Proceedings of the 58th Annual Meeting of the ACL*, pp. 7871-7880 https://doi.org/10.18653/v1/2020.acl-main.703.

[7] A, Radford et al. (2019). Language models are unsupervised multitask learners. *OpenAI blog*, 1, pp. 9.

[8] H, Touvron et al. (2023). LLaMA: Open and efficient foundation language models. *arXiv preprint*, pp. arXiv:2302.13971 https://doi.org/10.48550/arXiv.2302.13971.

[9] J, Achiam et al. (2023). GPT-4 technical report. *arXiv preprint*, pp. arXiv:2303.08774 https://doi.org/10.48550/arXiv.2303.08774.

[10] L, Ouyang et al. (2022). Training language models to follow instructions with human feedback. *Advances in Neural Information Processing Systems*, 35, pp. 27730-27744 https://doi.org/10.48550/arXiv.2203.02155.
